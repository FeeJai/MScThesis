\chapter*{Abstract}

\begin{center}
\begin{minipage}[t]{.8\textwidth}
\setlength{\parskip}{.5\baselineskip}


Gathering data from audio recordings relies primarily on hand-crafted and fine-tuned signal processing methods. In state of the art applications, the output is then used with machine learning algorithms. Most widespread is speech recognition (e.g. Siri) while other applications such as understanding the background noise environment are currently underrepresented. That is surprising as this information could lead to significantly improved situational awareness in stressful situations and thereby might improve traffic safety.

%A common example is the Mel Frequency Cepstral Coefficients \cite{1168654}, a voice pitch independent representation of the time variant properties in the speech spectrum \cite{Hanson1996}.

In this thesis, I have evaluated current developments in machine learning (deep neural networks) and researched their applicability for urban sound classification. Most literature in this field has been published by the 'Sounds of New York City' project and Google's Machine Perception research team with an overview given in chapter \ref{chap:dataset}.
 
When testing novel neural network architectures I however encountered the latest (unfortunate) trend: a lot of AI research publications cannot easily be replicated. This is why chapter \ref{chap:other} is about testing assumptions and discusses reasons as well as possible solutions.

\end{minipage}
\end{center}
